<img width="1311" height="520" alt="image" src="https://github.com/user-attachments/assets/0bfb7368-1c34-4a4c-9c5a-b2e8b755fa04" />

这份思维导图围绕《AI绘画核心技术与实战》展开，分为4大核心模块，具体内容整理如下：


### 一、绘画工具
包含**绘画能力**和**绘画技巧**，并提及更多工具：
- **绘画能力**：文生图/图生图、图片延展/局部重绘、风格迁移等
- **绘画技巧**：Prompt高阶用法（标签、负面提示词、文本权重）、LoRA用法、重要参数使用技巧等
- **更多工具**：CivitAI、HuggingFace、Midjourney


### 二、经典模型
解析主流AI绘画模型的技术要点：
- **DALL-E 2**：unCLIP、图像魔改、扩散先验
- **Imagen**：CLIP vs T5、Text-in-Image、进阶版DeepFloyd模型
- **Stable Diffusion深入解析**：SD模型演化、UNet与注意力机制、SD reimagine原理、SDXL技术方案
- **Midjourney**：技术方案推测、发展趋势预测


### 三、生成基础
讲解AI绘画的底层技术原理：
- **扩散模型**：加噪与去噪、训练与推理、基本理论、与GAN的对比、重要改进
- **核心模块**：Transformer、UNet（扩散模型核心）、采样器、VAE系列、CLIP模型
- **GAN**：基础原理与局限、应用场景解读


### 四、进阶算法
覆盖内容定制、图像编辑、风格化等进阶技术：
- **内容定制**：DreamBooth、LoRA、ControlNet
- **图像编辑**：inpainting模型、Prompt2Prompt、Null-Text Inversion
- **风格化**：图像风格化、视频风格化技巧


### 五、AI绘画延展
探索AI绘画的拓展应用方向：
- AIGC数字人技术、3D内容生成、AI视频生成等

漫画ip形象

线稿上色

图像风格化

创意头像

老照片修复

ai会话还能给我们的画作补充细节，让画面更加生动细致

智能修图

ai绘画模型的图像编辑能力，修图更加精确快速，

视频逐帧画面的风格化的处理，

创意广告

ai画师可以根据给定的文本描述，目标受众和品牌风格，自动生成和之匹配的视觉元素，色彩和构图，

图像变体

制作贴纸

杂志封面生成

二维码生成

创意图像生成，ip形象设计，图像风格化，创意头像，老照片修复，智能修图

生成创意广告，图像变体辅助设置，制作专属贴纸，杂志大片。个人二维码

stable diffusion，任何一张图像可以通过不断添加噪声变成一张完全被噪声覆盖的图像，任何一张噪声图像通过逐步合理的去除噪声，变得清晰可辨

img2img:图生图，不同的风格

输入图像经过添加噪声的处理，变成了一个不清晰的噪声图，然后结合不同的prompt语句，图像逐渐变得清晰，并且呈现出了prompt语句相关的风格

每一步减少噪声的影响，使得图像细节逐渐清晰，并且呈现出和prompt语句相匹配的风格特征

延展图像

扩展图像的边界或者填充缺失的区域，


将原始图像当中的图像之外的区域看作不清晰的噪声图，但是会保持原本区域不变，并且参考原始图像的信息来生成原图之外的内容

inpainting：局部重回

在图像内部绘制一个遮罩，使用webui自带的画笔功能可以实现

对要重新画的区域进行遮罩后。使用不同的prompt语句，就可以为改遮罩重新绘制新的内容，可以根据所选择的风格和要表达的意图，在图像的特定的区域上添加或者修改内容，从而实现更加具有创意和个性化的图像的效果

希望强调prompt语句当中的特定的词汇，为了实现这个目的，圆括号，（）来突出显示这些词汇，


face restoration 面部修复

对生成图像的当中的人脸区域进行修复，

文字圣徒，图像生图，图像延展，局部重回，面部修复，风格迁移


提升画的质量，使用标签继续优化，

（）特殊语法来增强提示词

在prompt当中添加()默认情况下会让对应的单词产生1.1被的强度，双括号（（））则表示1.1*1.1被的加强，当然我们也可以直接将数字写上去

不建议权重超过1.3，否则会对画面的影响很大，深圳不能产生正常的图像

<lora:模型文件名：权重>

权重范围是0到1，其中0表示lora模型完全不起作用，webui会自动加载响应的lora模型，并且根据权重大小进行应用，这些lora文件可以是自己训练的，

cfg scale的范围是1-30默认是7.调整不同的scale值来观察图像的变化，

prompt可以用来引导模型生成特定的风格，内容或者特征的图像，

直接描述，巧用标签，负面提示词，文本权重调整，引入lora

提示词相关性，cfg scale：作用是控制输入文本对生成图像的影响成都，

范围是1-30，默认值是7，。

较高的cfg scale只可以让生成的图像更加贴近prompt的信息，但是并不是越高越好的



面部手部瑕疵如何处理：

1.sd当中开启面部修复

2.蒙版局部重回

3.利用sd插件after detailer最推荐

图生图需要输入prompt和原始图像，首先原始图像会经过加噪处理，转变成为stable diffusion 模型可以处理的噪声图像，

仔细控制参数，生成的图像就能够同时包含原始图像的特征和prompt语句相关的内容，用prompt引导原始图像产生新的图像


denoising strength(重绘强度）

重绘强度仅在图生图（img2img）或者高清修复的过程当中被应用，这个变量表示图像加噪的强度，代表了生成的图像相对于原始输入的图像的内容的变化的程度，数值越高，ai对原图的参考成都就越低，生成的图像和原图的区别就越大


重绘强度参数对图像生成的影响非常显著，较高的重绘强度可以帮助模型更好的理解并且生成和prompt相关的内容，同时保留原始图像的整体的结构，证明了参数对图生图的关键的作用。

重绘强度在图生图过程当中起到了重要的作用，生成的图像相对于原始输入图像的内容的变化程度，较高的重绘程度会导致生成的图像更大程度和prompt相关，同时有些偏离原始原图像的特征，效果就是生成的图像更加的创新，有趣，但是和原始图像的相似性较低

较低的重绘强度会更加倾向于保留原始图像的整体的特征和结构，让生成的图像更加接近于原始图像，但是在和prompt相关性方面稍显不足，只能针对原始图像做小修小布。


采样步数为30，重绘强度取0.6是不错的选择，更多的参数组合，

较低的重绘强度会更加倾向于保留原始图像的整体的特征和结构，让生成的图像更加接近原始图像，但是可能在和prompt相关性方面稍显不足的

nijijourney这个模型，大量的动漫图像进行微调后得到的二次元风格专属的模型，nigijourney模型进行图生图的任务

midjourney支持n图融合，n最大支持5张图

midjourney当前并不支持lora用法，负面描述词，文本权重调整技巧，负面描述词，可以通过--no来指定，文本权重调整，通过：：数值来指定。

重绘强度是图生图模型当中的一个关键参数，表示图像的重绘程度，较高的重绘强度可以让图像更加多样化，但是仍然保持原始图像的整体的轮廓。通过调整重绘强度，可以控制生成图像和原始输入图像的内容的变化程度，实现图生图的风格化，编辑等过程。

midjourney这三种常见的用法：文字生图，图像生图，多图融合，

midjourney：文字生图，图像生图，多图融合，负面描述词。文本权重

《lora：模型文件名：权重》权重通常是0到1，0表示lora模型完全不起作用，webui自动加载相应的lora模型，并且根据权重的大小进行应用/。

lora的权重很重要，权重越高，生成的图像就越倾向与lora模型所代表的独特风格

通过文生图的功能来验证基础的效果，确认没有问题以后，使用超分功能进行高分辨率处理，

对超分的参数进行设置，比如上采样倍率设置为2，将分辨率提升为一倍的，

lora的本质是冻结住stable diffusion模型的参数，重新微调很少一部分的模型的参数，使用这部分的参数对原始的stable diffusion模型参数进行干预，lora的微调的质量和全模型微调相当的，



连接不同的功能的lora模型，根据创作的需求，可以选择风格化的或者人物化的lora，风格化的lora可以用于生成具有特定的风格的图像，而人物化的lora则适用于创建独特的任务的形象

选择合适的lora权重，权重越高，生成的图像越接近lora模型的效果

lora的标准用法：lora：模型文件名：权重


gan生成对抗网络是唯一的选择，扩散模型技术逐渐成为ai会话的主流技术，扩散模型似乎要优于gan



深度神经网络通常需要收集图像样本和目标标签比如分类任务的标签就是类别信息，年龄回归任务的标签就是年龄数值，通常通过交叉熵损失来训练分类任务，通过数值误差损失，比如l1损失和l2损失来训练回归任务。

gan的思路王权不同，gan模型由两个模块构成的，生成器generator和判别器discriminator，可以这样类比，，生成器是一位名画伪造家，目标是创作出逼真的艺术品，判别器是一位艺术鉴赏家，目标是从细节当中找出伪造破绽，生成器和判别器在模型训练的过程当中要持续更新和对抗，最终达到平衡


```
for epoch in range(num_epochs):
for batch_data in data_loader:
# 更新判别器
real_images = batch_data.to(device)
z = torch.randn(batch_size, latent_dim).to(device)
fake_images = generator(z).detach()
d_loss_real = discriminator(real_images)
d_loss_fake = discriminator(fake_images)
# 判别器损失
d_loss = -(torch.mean(d_loss_real) - torch.mean(d_loss_fake))
discriminator.zero_grad()
d_loss.backward()
discriminator_optimizer.step()

# 更新生成器
z = torch.randn(batch_size, latent_dim).to(device)
fake_images = generator(z)
g_loss = -torch.mean(discriminator(fake_images))
generator.zero_grad()
g_loss.backward()
generator_optimizer.step()
```
在每一个训练周期，对于每个批次的数据是这样处理的。

首先更新判别器，将真实图像和生成器生成的假图像输入到判别器当中，计算真实图像的损失和生成图像的损失，通过反向传播更新判别器的参数，也就是利用梯度下降类的算法更新模型的权重

接着更新生成器，生成一批随机噪声输入到生成器当中生成图像，再将生成的图像输入到判别器当中计算损失，之后反向传播更新生成器的参数，


重复以上的步骤进行多个训练周期，知道达到预定的训练次数

判别器的目标是区分真是图像和生成图像，损失函数的设计是最大化真实图像的损失d_loss_real和最小化生成图像的损失d_loss_fake

torch.mean(d_loss_real)计算了真实图像的平均损失而torch.mean(d_loss_fake)计算了生成图像的平均损失，用减号叫两个损失详见，为了实现最大化真实图像

torch.mean(d_loss_real)计算了真实图像的平均损失，而torch.mean(d_loss_fak)计算了生成图像的平均损失，减号将两个损失相减，为了实现最大化真实图像损失和最小化生成图像的损失的效果，我们希望判别器能够更好的区分真实图像和生成图像，提供生成器生成逼真图像的能力

gan，对抗训练的思想，gan通过生成器和判别器的竞争学习，使得生成的图像逐渐趋近于真实图像，gan的应用场景广泛，

gan存在一些问题，同时训练生成器和判别器的过程不稳定，生成器生成的内容不能被指定，生成的图像的分辨率低，模型推理在手机等设备上用时过长，gan发展经历了一系列的重要改进，



gan

最初的gan并没有因为GAN模型存在一些问题，比如同时训练生成器和判别器的过程并不稳定，最初的生成器生成内容不能被指定，生成的图像分辨率较低，模型推理在手机等设备上用时过长等等。



图像生成能力的进化：DCGAN/CGAN/WGAN
最初的GAN模型使用全连接神经网络，对于图像生成任务来说，学习图像的空间结构和局部特征是非常困难的。

但2015年由Radford等人提出的深度卷积GAN（DCGAN）给GAN带来了进化可能。主要创新就是引入卷积神经网络（CNN）结构，通过卷积层和反卷积层替代全连接层，使得生成器和判别器能够感知和利用图像的局部关系，更好地处理图像数据，从而生成更逼真的图像。

DCGAN的优点在于它的稳定性和生成效果。通过使用卷积神经网络，DCGAN能够更好地保持图像的空间结构和细节信息，生成的图像质量更高。此外，DCGAN的架构设计也为后续的GAN改进工作提供了重要的基础。


条件GAN，简称cGAN，允许我们在生成图像的过程中引入额外的条件信息。这样一来，我们可以控制生成图像的特征，比如生成特定类别的图像。比如在上面的数字图中，普通的GAN无法提前指定生成的数字是0到9中的哪一个，而cGAN便可以轻松控制要生成的数字是几。


Wasserstein GAN，简称wGAN，是另一个重要的改进，它通过使用Wasserstein距离（瓦瑟斯坦距离，也被称为地面距离）来衡量生成图像和真实图像之间的差异，这样就能提升训练的稳定性和生成图像的质量。

Wasserstein距离用于比较两个概率分布之间的差异，量化了将一个分布转换为另一个分布所需的最小工作量。

这么说有点抽象，我再举个形象的例子帮你理解，假设我们有两堆沙子，一堆沙子分布在一个地方，另一堆沙子分布在另一个地方。现在我们想将第一堆沙子移动到第二堆沙子的位置，但我们只能以一定的速度和固定的容器大小来移动沙子。Wasserstein距离就是将第一堆沙子移动到第二堆沙子所需的最小总移动成本。在wGAN中，这两堆沙子就是真实数据分布和生成数据分布。


cGAN和wGAN生成图像的分辨率很低，分辨率提升是图像生成领域一个持续研究的方向，后来的 PGGAN、BigGAN、StyleGAN 等工作，将生成图像的分辨率提高了1024x1024分辨率之上。这个我们之后再讲。

手机端实时特效：从Pix2Pix到CycleGAN

Pix2Pix 系列工作延续了cGAN的思想，将cGAN的条件换成了与原图尺寸大小相同的图片，可以实现类似轮廓图转真实图片、黑白图转彩色图等效果。是不是听起来很熟悉？没错，就是GAN时代的ControlNet！


Pix2Pix最大的缺点就是训练需要大量目标图像与输入图像的图像对，优点是模型可以做到很轻很快，甚至能在很低端的手机上也能达到实时效果。从18年至今，我们在短视频平台上看到的各种实时变脸特效，比如年龄转换、性别编辑等特效，都是基于这个技术。

那么问题来了，获取成对的数据是困难且耗时的，那大量成对数据该怎么来呢？答案就是大名鼎鼎的 CycleGAN。2017年Jun-Yan Zhu等人提出了CycleGAN，也就是循环一致性生成对抗网络。


CycleGAN的核心要点就是让两个不同领域的图像可以互相转换。它有两个生成器，分别是G（A→B）和G（B→A），它们的任务是把A领域的图像变成B领域的，反之亦然。同时，还有两个判别器，D_A和D_B，负责分辨A和B领域里的真实图像和生成的图像。

CycleGAN的关键点在于循环一致性损失。这个方法把原图像转换到目标领域，然后再转换回原来的领域，就可以确保生成的图像跟原图像差别不大。这种循环一致性约束让图像转换有了双向的一致性。我举个例子你就明白了，先把马变成斑马，再恢复成马，最后的图像应该跟原来的马图像很相似。

CycleGAN的优势是不需要成对的训练数据便可以实现图像转换，在很多图像转换任务上都表现得非常出色，比如风景、动物、风格等转换。再加上Pix2Pix，CycleGAN简直是制作短视频特效的神器。

高分辨率的生成：StyleGAN系列工作
之后，英伟达在2018年提出的生成对抗网络模型StyleGAN，彻底改变了GAN在图像合成和风格迁移方面的应用前景。与传统的GAN模型相比，StyleGAN在图像生成的质量、多样性和可控性方面取得了显著的突破。

StyleGAN的核心思想是用风格向量来控制生成图像的各种属性特点，并通过自适应实例归一化（AdaIN）把风格向量和生成器的特征图结合在一起。另外，用渐进式的生成器结构逐渐提高分辨率，这样可以提高训练的稳定性和生成图像的质量。


StyleGAN的应用非常广泛。它不仅可以用于生成高分辨率的逼真图像，还可以用于风格迁移、图像编辑和人脸合成等任务。StyleGAN生成的图像质量非常高，具有细致的纹理、自然的细节和丰富的变化，可用于各种创作、设计和研究领域。

StyleGAN 2和StyleGAN 3是StyleGAN的改进版本。它在StyleGAN的基础上引入了一系列重要的改进，进一步提升了图像生成的质量、稳定性和控制性。

另外还有一种叫做超分辨率生成对抗网络（SRGAN）的模型，它的目标是将低分辨率图像转换成高分辨率的图像。

讲到这估计你也发现了，GAN类型的生成模型非常多，我这里给你分享的是最有影响力的模型，对于其他GAN模型，有兴趣的话你可以了解下BigGAN、StarGAN、Progressive GAN等模型。

# GAN的应用场景
无论过去还是现在，在图像生成、编辑和风格化领域，GAN都占据着非常重要的地位，而且是生成模型发展的重要里程碑。

图像生成

GAN可以生成各种类型的图像，包括自然风景、人脸、动物等。通过训练生成器网络，GAN能够从随机噪声中生成逼真的图像，为艺术创作、虚拟场景生成、游戏开发等领域提供了强大的工具。


图像局部编辑


GAN可以通过生成器网络实现对图像局部的编辑。通过将输入图像和编辑向量结合，可以精确地控制生成器网络，在特定区域编辑图像，比如改变图像的颜色、纹理或形状。这为图像编辑和修复提供了一种更加灵活和高效的方式。

图像风格化


GAN可以将图像转换为具有不同艺术风格的图像。通过训练一个生成器网络，可以将输入图像转换为特定风格的图像，如印象派、油画、水彩画等。这种图像风格化技术，广泛应用于艺术创作、图像处理和社交媒体滤镜等领域。

老照片修复


另外，GAN可以用于修复老照片中的损坏或模糊的部分。通过训练生成器网络，GAN可以学习恢复损坏图像的细节和纹理，并生成高质量的修复结果。这在数字文化遗产保护和历史文档修复等领域具有重要的应用意义。

# 与扩散模型狭路相逢
尽管GAN逐渐走向高光，高分辨率生成、可控编辑能力等问题也得到了解决，GAN仍然存在着局限性。GAN的局限性主要表现在训练不稳定性、生成图像模糊、难以评估和控制生成质量等问题。此外，在图像风格化、图像编辑等任务中，通常是每个任务一个GAN。训练成本、数据需求量、使用场景局限性都是实际工作中的痛点。

而扩散模型在很大程度上解决了GAN的痛点。其实扩散模型并不是这两年的新鲜事，实际上，早在2015年就有人提出了图像扩散模型的概念。而GAN是2014年！二者几乎是前后脚同时提出的。

2021年之前GAN一直在图像生成领域处于制霸地位，直到2021年10月，一篇名为“扩散模型在图像生成领域击败了GAN” 的文章横空出世，扩散模型在图像生成领域的潜力才广为人知。

后来OpenAI的Glide、DALL-E 2，Google的Imagen、Parti，还有广为人知的Stable Diffusion、Midjourney，更是把基于扩散模型的AI绘画推向了新的高度。关于扩散模型，下一讲我们再深入探讨。

# GAN能否东山再起？
有意思的是，热衷于GAN的研究人员并没有放弃。就在2023年3月，Adobe的学者提出了GigaGAN, 一个新的GAN架构。一听这个名字，就有一种大模型的味道。

GigaGAN是一种具有突破性的GAN模型，它通过扩大模型规模，在多个方面展现了卓越的优势。比如，对于512分辨率图像的合成，仅需要0.13秒的推理速度，这比现有的工作在推理速度上高出了一个数量级。并且GigaGAN可以合成更高分辨率的图像，生成1600万像素的图像仅需3.66秒。

我们再来看看GAN领域的另一位明星——DragGAN。实际上，DragGAN是一种交互式图像操作方法，为各种GAN开发提供了一种神奇的功能，我们用鼠标简单拉伸图像，就能够生成全新的图像。

使用DragGAN非常简单，用户只需要设置一个起始点、一个目标点，以及希望修改的区域。接下来，模型会进行运动监督和点跟踪这两个步骤的迭代，然后修改原始图像。这种交互式的操作方式让图像的编辑变得非常直观和有趣。

<img width="1313" height="1039" alt="image" src="https://github.com/user-attachments/assets/9b9746fd-5eea-4f8b-b7fd-93851ce21c69" />



