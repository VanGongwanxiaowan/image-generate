https://aigcreative.feishu.cn/wiki/VTqgwLTziiK8H9kJuUXcH0kMnTh

# ComfyUI详细入门系列教程总结
## 一、ComfyUI核心优势与基础信息
### （一）核心优势
1. **显存占用低**：在8GB VRAM的GPU上可有效运行SDXL 1.0模型，而sdwebui需10GB VRAM才能运行该模型。
2. **工作流灵活**：基于节点设计，支持可视化搭建，可快速切换工作流，还能自行设计或导入预设工作流。
3. **运行高效**：速度快，能满足快速生成图像的需求。
4. **插件丰富**：支持ADetailer、Controlnet、AnimateDIFF等多种插件，且有200多类可在节点管理器自行安装的节点（但不建议全装，避免冲突，按需安装即可）。

### （二）与stable-diffusion-webui关联
1. **功能相似**：二者都可用于AI图像生成相关操作。
2. **安装类似**：提供的特别压缩包解压后即完成安装，还可通过修改`extra_model_paths.yaml`（将`extra_model_paths.yaml.example`重命名并修改模型目录），实现自定义模型目录或与stable-diffusion-webui共用模型目录。

## 二、ComfyUI安装与初始设置
1. **获取安装包与模型**：下载ComfyUI安装包和SDXL 1.0的base、refiner两个模型。
2. **安装步骤**：解压安装包，将SDXL模型放在指定模型路径，安装插件管理器（进入命令行模式，在ComfyUI目录输入指令克隆插件管理器），安装必要插件后重启ComfyUI，最后导入工作流即可使用。
3. **界面优化**：通过提供的下载包，可获得管理器和汉化包，切换语言后界面更友好；也可安装浏览器翻译插件，翻译节点说明。

## 三、ComfyUI基础操作
### （一）节点操作
1. **添加节点**：可在空白区域双击鼠标左键弹出搜索窗来添加节点。
2. **节点安装原则**：按需安装，工作流缺什么节点就装什么，避免贪多导致冲突或功能冗余。

### （二）工作流使用
1. **基础工作流调用**：通过“加载默认”可调用最基础工作流，该工作流遵循SD模型基本运行逻辑，即文本/图形prompt输入—潜空间（latent）解码流程。
2. **工作流来源**：一半靠收集（有部分优质工作流不轻易外传），一半靠自己设计。
3. **工作流修改**：预设好的工作流，只需修改关键参数，点击任务排队执行即可生成图像；也可对基础工作流进行改动，如将文生图工作流改成图生图工作流。

## 四、专项功能教程
### （一）文生图与图生图
1. **文生图工作流**：加载模型后，绿色正向文本、红色负向文本与紫色空Latent（带目标尺寸和批次）一起流向K采样器，设置种子、步骤数等参数后，生成的潜在图像传递到VAE解码器，解码成可视图像，核心是将文本作为模型参考生成图像。
2. **图生图工作流**：在文生图工作流基础上加入图像参考，让图像变成latent的输出，本质是将图像信息和文本信息共同作为模型参考生成图像。

### （二）LORA接入
1. **节点与流程**：先加载主模型，再加载LORA模型（可加载多个，需设置模型强度、clip强度等参数）；文本编码时，将加载LORA后的CLIP信息与文本编码CLIP链接；编码后的文本提示传递到KSampler，设置相关参数后，生成的潜在图像经VAE解码器解码成图像，流程与一般文生图模型类似。
2. **便捷工具**：社区节点库中有如CR LORA STACK这样的节点，一个节点可选择3个LORA且带有开关，方便添加多个LORA。

### （三）高清放大
1. **常见方法**
    - **潜空间放大**：在ksampler和VAE之间增加upscale latent模块，可实现图像1.5倍放大。
    - **外置模型放大**：流程与潜空间放大不同，放大步骤在VAE decoder之后，先生成低分图像，再基于该图像用外置模型放大。
2. **高级玩法**
    - **TILED放大**：集成到单个节点，可增加图像细节。
    - **多模型融合放大**：用两个模型分别放大图像，再通过imageblend融合算法合成，最大限度保留或增加细节，此操作在webui中难实现。
3. **与webui放大功能关联**：webui中有EXTRAS（附加功能菜单）放大（优点是易操作，缺点是难解决细节缺失）和HIRES.FIX放大（生成阶段实现高质量生产，引入潜变量高分，2倍以下放大效果佳，常与EXTRAS结合实现4K级别大图），ComfyUI有专门流程实现类似高清放大功能。

### （四）Controlnet使用
1. **Controlnet效果差异**：OpenPose对体现文字提示效果最好，能保留姿势给AI更大发挥空间；Depth次之，保留人体整体轮廓和部分衣服深度信息质感；Lineart保留信息最多，包括头发光影细节甚至空中雪花。
2. **组合使用**：与webui思路相同，可联动使用多个Controlnet，一般2个足矣，3个为极限；需明确开启不同Controlnet组合的目的，且要关注各Controlnet的权重参数，可根据需求调整不同阶段各Controlnet的权重。

### （五）IP-Adapter使用
1. **模型安装与分类**
    - **安装路径**：在`/ComfyUI/models/clip_vision`目录安装CLIP相关模型，在`/ComfyUI/models/ipadapter`目录（不存在则创建）安装IP-Adapter模型，部分FaceID型号需在`/ComfyUI/models/loras`目录安装对应的LoRA。
    - **模型分类**：分为早期不带PLUS（基本弃用）和带PLUS（目前主力）两类，带PLUS的又分普通PLUS（用于风格迁移）和带face（主攻面部特征，用于换脸），且根据SD模型体系分为1.5体系和XL体系，分别适配两代模型。
2. **版本差异**：3月份COMFYUI中IP-Adapter代码全面重写，3月前为旧版，3月后为新版，新版节点更简洁，无需自行设置模型搭配，节点内已配置好各种用法搭配。
3. **使用方法**：可在IP-Adapter节点文件夹找到example示例，导入示例工作流查看不同搭配用法；其好处是无需训练风格LORA（效果越强，训练LORA必要性越低），且在工作流中有多种玩法，还可与Controlnet结合（IP-Adapter用于风格，Controlnet用于构图）实现视频风格转换。
4. **资源获取**：常用工作流可在openart.ai找到（访问需“魔法”），且IP-Adapter是频繁更新的技术模块，需保持对其更新的了解。
