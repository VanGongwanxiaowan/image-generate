# 第1章
## 视觉-语言模型基础：概念与研究路线图


### 摘要
视觉-语言模型通过打通视觉与文本理解的壁垒，极大推动了人工智能领域的发展。这类模型已支撑起广泛的应用场景，包括图像识别、目标检测、场景理解、2D与3D视觉内容的生成与编辑，以及视觉问答等。

本章系统介绍了视觉-语言模型的核心基础概念，重点阐述其通过新型神经网络架构与大规模数据预训练，学习多模态特征表示的独特能力。我们深入剖析了视觉-语言建模的范式，梳理了特征对齐、模型规模化、数据与评估等关键挑战，并回顾了领域内的重要研究进展。

此外，本章还讨论了现有方法的局限性，涵盖计算效率不足与伦理风险等问题，并展望了未来的潜在研究方向。本章可作为一份研究路线图，帮助读者理解该领域的核心原理，以及其在人工智能应用中的变革性潜力。

# 1.1 引言
视觉与语言是人类智能中深度交织的两项能力。

在人工智能（AI）研究中，这两项能力传统上被划分在不同领域——即计算机视觉与自然语言处理。

具体而言，计算机视觉专注于解读图像，例如识别图像中的物体[1, 2]或定位其像素级位置[3, 4]；

而自然语言处理则旨在分析与生成语言，例如预测客户评论的情感倾向[5]，或总结新闻文章与书籍内容[6]。


从本质上看，人类并非仅通过单一模态学习概念。

相反，学习过程往往涉及视觉与语言的交互。

例如，儿童学习“苹果”这一概念时，通常会同时接收视觉与语言信息：家长可能会展示真实的苹果或苹果图片，并说出相关语句，如“这是苹果”“你想吃苹果吗”“它是红色、圆形、甜甜的”；

随着时间推移，这一过程会在不同场景（如用餐或睡前故事时）重复，最终孩子会将苹果的视觉特征与语言标签建立关联。

此外，现有认知研究表明，儿童通过日常交流在词语与视觉场景间建立联系来习得概念[7, 8]。

因此，融合视觉与语言是构建人工通用智能（AGI）的自然路径。



大型视觉-语言模型（VLM）的出现彻底改变了AI研究格局，催生了众多全新应用，例如检测任意类别的物体[9]、根据文本描述生成逼真图像[10]，或用自然语言指令让机器人完成抓取[11]、导航[12]，甚至更复杂的手术操作[13]。

与2010年代开发的早期“视觉-语言模型”[14–16]相比，现代VLM在模型架构与训练数据规模上都有了大幅提升[17–21]。

例如，现代VLM的参数量呈指数级增长，从最初的数百万扩展到数亿甚至数十亿；

训练数据规模也达到了数十亿样本级别[22]，这已成为训练商业模型的标准做法。

研究表明，正是现代VLM的巨大规模（包括参数量与训练数据量），使其能够有效学习丰富且可泛化的世界知识[23, 24]。

借助从海量数据中学习到的广泛知识，VLM具备强大的通用性，可适配各类下游任务，涵盖判别式与生成式任务，甚至从2D延伸至3D领域。



尽管现代VLM的规模带来了前所未有的能力，但这种规模也带来了算法、计算与数据层面的重大挑战。

从算法角度，设计能够融合视觉与语言的架构并非易事。

此外，在将VLM适配到下游应用时，还需处理任务特定设计与模态差异带来的问题，例如将视觉与语言和人体姿态[25]或动作[26]关联。

在计算层面，如此规模的VLM训练需要巨大的计算资源，这限制了模型的实际广泛应用。

为应对这一挑战，高效训练技术与适配方法（如提示学习[27, 28]）的进步至关重要。

在数据层面，构建与管理十亿级别的训练数据需要解决数据噪声、偏差与多样性等问题，以确保模型学习的鲁棒性与安全性。



本章将介绍VLM的基础概念，重点关注特征对齐、可扩展性、数据与评估。

这些概念旨在为读者理解本书后续章节的问题与算法提供坚实基础。

本章结构如下：1.2节探讨视觉-语言建模范式的演变；

1.3节重点分析特征对齐、可扩展性、数据与评估等关键挑战；

1.4节回顾代表性研究进展，并对当前发展现状提供见解。



---



# 1.2 视觉-语言建模范式

本节旨在介绍视觉-语言模型（VLM）背后的学习范式背景，具体讨论范式演变、核心组件、主流模型架构，以及学界广泛使用的训练目标与策略。



## 1.2.1 范式演变

VLM的兴起很大程度上得益于预训练技术的进步，预训练已成为现代AI系统的基石。

预训练是指在大规模数据上针对通用任务（如预测文本中的掩码词[29]或图像中的缺失块[30]）训练模型，使模型学习数据中的通用模式、结构与关联，从而生成可广泛适配不同下游任务的特征表示。



### 视觉模型的演变

计算机视觉领域预训练技术的兴起，技术上始于ImageNet[31]这一大规模基准数据集，它推动了视觉识别领域的诸多突破。

AlexNet基于卷积神经网络（CNN）的成功标志着一个转折点[1]，它通过在ImageNet上刷新性能纪录，证明了深度学习的强大能力。

随后，ResNet[2]通过残差连接解决了棘手的梯度消失问题，使网络能够更深、更具表达力。

近年来，视觉Transformer（ViT）[32]通过将图像块视为序列，并以全自注意力方式学习特征表示，重新定义了该领域，在各类视觉任务上取得了最优性能[33–36]。



这些模型的发展依赖于海量数据集与计算能力的持续增长，二者共同支持了更复杂模型的规模化训练。

本质上，这些预训练模型出色的泛化能力，标志着现代计算机视觉发展的大趋势。

随着模型不断突破视觉识别的边界，也引发了对模型架构与学习范式的重新思考。

借助无监督与自监督学习技术的进步[37–44]，视觉领域开始探索在海量无标注数据上预训练模型，这不仅拓展了视觉模型的适用范围，也为VLM的诞生奠定了基础。



### 语言模型的演变

自然语言处理（NLP）领域的预训练发展晚于计算机视觉，但从早期词表示模型到大型语言模型的演变，不仅彻底改变了NLP，还对视觉领域产生了深远影响，如前文提到的ViT模型就是例证。



最初，Word2Vec[45]与GloVe[46]等基础模型将单词表示为稠密的固定向量，通过词共现模式捕捉语义关联。

随后，学界转向以ELMo[47]为代表的上下文嵌入模型。

ELMo基于双向LSTM生成动态词表示，通过考虑单词的上下文解决了静态嵌入的局限，使模型能根据周围词语生成不同的向量表示，显著提升了需要细粒度语义理解任务的性能。



序列到序列（seq2seq）模型[48]的出现，以及后续Transformer[49]的发展，带来了又一关键变革。

基于编码器-解码器架构的seq2seq模型，在机器翻译等任务上取得了显著进展，它能将整个词序列编码为固定大小的向量，再解码为另一个序列。

但依赖循环层的设计削弱了seq2seq模型处理长距离依赖的能力。

为解决这一问题，Transformer用自注意力机制替代了循环结构，能更好地捕捉长序列关联，并通过并行化实现更快训练。

典型的语言Transformer包括BERT[29]与GPT[50]，分别基于掩码语言建模与自回归学习（即下一词预测）。



### 视觉与语言模型的融合

视觉与语言领域预训练模型的进步，极大推动了VLM的发展——这类模型在统一框架中集成了视觉与语言两种模态。

早期的开创性工作（如DeViSE[15]）试图通过共享嵌入对齐视觉与文本特征，以弥合模态鸿沟，但这些模型受限于可扩展性不足，无法处理大规模、多样化的数据集。



CLIP[17]与ALIGN[51]等模型的出现彻底重塑了VLM格局，它们通过大批次对比学习训练双编码器架构，生成图像与文本的联合特征表示。

这些模型在零样本识别任务中展现出前所未有的泛化能力，解锁了图像分类、检索，甚至文本生成图像[10]等多样化下游应用的潜力。



随着Flamingo[20]与GPT-4V[52]等更强大、更通用的VLM问世，该领域持续发展。

例如，Flamingo的构建方式是：先将预训练视觉模型与预训练语言模型结合，再用图像描述数据微调组合后的模型。

最终模型在图像描述、视觉问答等感知与推理任务上表现出色，凸显了其融合视觉与语言的能力。

GPT-4V（推测以类似方式构建）则展现出更强大的感知与推理能力。

随着VLM的持续演进，它们有望创造出更直观、更通用的AI系统，使其能更像人类一样理解和与世界交互，以更贴近人类认知的方式处理和关联视觉与语言信息。



## 1.2.2 核心组件

本节讨论VLM处理、对齐与融合视觉和文本信息的主要构建模块，分为特征表示、特征对齐与融合、训练目标和数据四类。


### 特征表示

现代VLM依赖专用架构从图像与文本中提取紧凑特征。



**视觉方面**，常用的骨干网络包括擅长提取空间层次特征的CNN（如ResNet[2]），以及将图像视为块序列、通过自注意力捕捉全局依赖的ViT[32]。

这些骨干网络通常在ImageNet[31]等大规模数据集上预训练，以学习通用视觉特征。



**语言方面**，基于Transformer的架构占据主导地位，已被证明在处理序列数据时极为高效[49]。

语言模型既可与视觉模型联合训练，也可先在文本数据集上通过自监督学习预训练，再与视觉模型结合进行进一步微调。

例如，在CLIP[17]中，语言模型是与视觉模型在海量图像-文本对数据上联合训练的Transformer；

而Flamingo[20]与LLaVA[18]等模型则采用预训练语言模型，并将文本输出与视觉特征关联。



### 特征对齐与融合

弥合视觉与语言的鸿沟需要专用机制来对齐与融合不同模态的特征，主流架构有三类：

- **双编码器架构**：使用独立编码器分别处理视觉与文本信息，并通过对比学习等方式在共享嵌入空间中对齐两种模态（如CLIP[17]）。

- **交叉注意力架构**：实现视觉与文本特征的直接交互，使模型能对两种模态进行联合推理。交叉注意力通常应用于语言Transformer的中间自注意力层，代表工作包括Flamingo[20]与InstructBLIP[53]。

- **统一架构**：在同一Transformer模型中处理两种模态（如UNITER[54]）。具体做法是将图像与文本都进行标记化，再将其特征拼接成一个序列。



### 训练目标

训练目标在对齐不同模态的过程中起着关键作用，主流选择包括：

- **对比学习**：通过最大化图像-文本对在联合嵌入空间中的相似度来对齐二者（如CLIP[17]）。

- **掩码建模**：代表性方法包括掩码图像建模[30]与掩码语言建模[29]，核心思想是训练模型预测两种模态中的掩码标记（如UNITER[54]、BLIP[55]与VideoBERT[56]）。

- **自回归学习**：类似语言建模，目标是根据前文标记预测下一个标记。在VLM中，该范式常结合图像描述数据使用，即训练模型为图像生成描述（如LLaVA[18]与Flamingo[20]）。



### 数据

数据是VLM的基础，为模型提供学习世界知识的必要信息。

关键数据类型包括图像-文本对、视觉问答（VQA）数据集、指令与对话数据，以及视频-文本数据。

每种数据类型针对特定能力（从描述、推理到时序理解），共同推动多模态学习的发展。

以下是更详细的讨论：

- **图像-文本对数据**：提供对齐的视觉与文本输入，用于学习多模态表示。中等规模数据集（如COCO Captions[57]、Flickr30k[58]）为数十万张图像提供人工撰写的描述；

  大规模数据集（如LAION-5B[22]）则包含数十亿个网络收集的图像-文本对。这些数据集支撑了图像描述、检索等任务，极大助力了CLIP[17]、ALIGN[51]等VLM（它们通过对比学习将图像与文本映射到共享空间）。

- **VQA数据**：VQA数据集（如VQA v2[59]、OK-VQA[60]、GQA[61]）通过让模型用自然语言回答图像相关问题，评估其图像推理能力。这些数据集推动模型整合视觉理解、物体关系与常识推理能力。


- **指令与对话数据**：这类数据连接感知与交互，支持多模态助手、任务导向推理等应用。例如，LLaVA-Instruct[18]提供基于视觉输入的多模态指令；

  Visual Dialog[62]引入对话式推理；RefCOCO/RefCOCOg[63]聚焦通过语言识别物体。
- **视频-文本数据**：视频-文本数据集将VLM拓展到时序领域，将视频与描述、标注或字幕配对。例如，HowTo100M[64]将教学视频与字幕配对，YouCook2[65]提供分步视频标注。

