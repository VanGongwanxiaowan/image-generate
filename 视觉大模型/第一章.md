# 第1章
## 视觉-语言模型基础：概念与研究路线图


### 摘要
视觉-语言模型通过打通视觉与文本理解的壁垒，极大推动了人工智能领域的发展。这类模型已支撑起广泛的应用场景，包括图像识别、目标检测、场景理解、2D与3D视觉内容的生成与编辑，以及视觉问答等。

本章系统介绍了视觉-语言模型的核心基础概念，重点阐述其通过新型神经网络架构与大规模数据预训练，学习多模态特征表示的独特能力。我们深入剖析了视觉-语言建模的范式，梳理了特征对齐、模型规模化、数据与评估等关键挑战，并回顾了领域内的重要研究进展。

此外，本章还讨论了现有方法的局限性，涵盖计算效率不足与伦理风险等问题，并展望了未来的潜在研究方向。本章可作为一份研究路线图，帮助读者理解该领域的核心原理，以及其在人工智能应用中的变革性潜力。

# 1.1 引言
视觉与语言是人类智能中深度交织的两项能力。

在人工智能（AI）研究中，这两项能力传统上被划分在不同领域——即计算机视觉与自然语言处理。

具体而言，计算机视觉专注于解读图像，例如识别图像中的物体[1, 2]或定位其像素级位置[3, 4]；

而自然语言处理则旨在分析与生成语言，例如预测客户评论的情感倾向[5]，或总结新闻文章与书籍内容[6]。


从本质上看，人类并非仅通过单一模态学习概念。

相反，学习过程往往涉及视觉与语言的交互。

例如，儿童学习“苹果”这一概念时，通常会同时接收视觉与语言信息：家长可能会展示真实的苹果或苹果图片，并说出相关语句，如“这是苹果”“你想吃苹果吗”“它是红色、圆形、甜甜的”；

随着时间推移，这一过程会在不同场景（如用餐或睡前故事时）重复，最终孩子会将苹果的视觉特征与语言标签建立关联。

此外，现有认知研究表明，儿童通过日常交流在词语与视觉场景间建立联系来习得概念[7, 8]。

因此，融合视觉与语言是构建人工通用智能（AGI）的自然路径。



大型视觉-语言模型（VLM）的出现彻底改变了AI研究格局，催生了众多全新应用，例如检测任意类别的物体[9]、根据文本描述生成逼真图像[10]，或用自然语言指令让机器人完成抓取[11]、导航[12]，甚至更复杂的手术操作[13]。

与2010年代开发的早期“视觉-语言模型”[14–16]相比，现代VLM在模型架构与训练数据规模上都有了大幅提升[17–21]。

例如，现代VLM的参数量呈指数级增长，从最初的数百万扩展到数亿甚至数十亿；

训练数据规模也达到了数十亿样本级别[22]，这已成为训练商业模型的标准做法。

研究表明，正是现代VLM的巨大规模（包括参数量与训练数据量），使其能够有效学习丰富且可泛化的世界知识[23, 24]。

借助从海量数据中学习到的广泛知识，VLM具备强大的通用性，可适配各类下游任务，涵盖判别式与生成式任务，甚至从2D延伸至3D领域。



尽管现代VLM的规模带来了前所未有的能力，但这种规模也带来了算法、计算与数据层面的重大挑战。

从算法角度，设计能够融合视觉与语言的架构并非易事。

此外，在将VLM适配到下游应用时，还需处理任务特定设计与模态差异带来的问题，例如将视觉与语言和人体姿态[25]或动作[26]关联。

在计算层面，如此规模的VLM训练需要巨大的计算资源，这限制了模型的实际广泛应用。

为应对这一挑战，高效训练技术与适配方法（如提示学习[27, 28]）的进步至关重要。

在数据层面，构建与管理十亿级别的训练数据需要解决数据噪声、偏差与多样性等问题，以确保模型学习的鲁棒性与安全性。



本章将介绍VLM的基础概念，重点关注特征对齐、可扩展性、数据与评估。

这些概念旨在为读者理解本书后续章节的问题与算法提供坚实基础。

本章结构如下：1.2节探讨视觉-语言建模范式的演变；

1.3节重点分析特征对齐、可扩展性、数据与评估等关键挑战；

1.4节回顾代表性研究进展，并对当前发展现状提供见解。



---



# 1.2 视觉-语言建模范式

本节旨在介绍视觉-语言模型（VLM）背后的学习范式背景，具体讨论范式演变、核心组件、主流模型架构，以及学界广泛使用的训练目标与策略。



## 1.2.1 范式演变

VLM的兴起很大程度上得益于预训练技术的进步，预训练已成为现代AI系统的基石。

预训练是指在大规模数据上针对通用任务（如预测文本中的掩码词[29]或图像中的缺失块[30]）训练模型，使模型学习数据中的通用模式、结构与关联，从而生成可广泛适配不同下游任务的特征表示。



### 视觉模型的演变

计算机视觉领域预训练技术的兴起，技术上始于ImageNet[31]这一大规模基准数据集，它推动了视觉识别领域的诸多突破。

AlexNet基于卷积神经网络（CNN）的成功标志着一个转折点[1]，它通过在ImageNet上刷新性能纪录，证明了深度学习的强大能力。

随后，ResNet[2]通过残差连接解决了棘手的梯度消失问题，使网络能够更深、更具表达力。

近年来，视觉Transformer（ViT）[32]通过将图像块视为序列，并以全自注意力方式学习特征表示，重新定义了该领域，在各类视觉任务上取得了最优性能[33–36]。



这些模型的发展依赖于海量数据集与计算能力的持续增长，二者共同支持了更复杂模型的规模化训练。

本质上，这些预训练模型出色的泛化能力，标志着现代计算机视觉发展的大趋势。

随着模型不断突破视觉识别的边界，也引发了对模型架构与学习范式的重新思考。

借助无监督与自监督学习技术的进步[37–44]，视觉领域开始探索在海量无标注数据上预训练模型，这不仅拓展了视觉模型的适用范围，也为VLM的诞生奠定了基础。



### 语言模型的演变

自然语言处理（NLP）领域的预训练发展晚于计算机视觉，但从早期词表示模型到大型语言模型的演变，不仅彻底改变了NLP，还对视觉领域产生了深远影响，如前文提到的ViT模型就是例证。



最初，Word2Vec[45]与GloVe[46]等基础模型将单词表示为稠密的固定向量，通过词共现模式捕捉语义关联。

随后，学界转向以ELMo[47]为代表的上下文嵌入模型。

ELMo基于双向LSTM生成动态词表示，通过考虑单词的上下文解决了静态嵌入的局限，使模型能根据周围词语生成不同的向量表示，显著提升了需要细粒度语义理解任务的性能。



序列到序列（seq2seq）模型[48]的出现，以及后续Transformer[49]的发展，带来了又一关键变革。

基于编码器-解码器架构的seq2seq模型，在机器翻译等任务上取得了显著进展，它能将整个词序列编码为固定大小的向量，再解码为另一个序列。

但依赖循环层的设计削弱了seq2seq模型处理长距离依赖的能力。

为解决这一问题，Transformer用自注意力机制替代了循环结构，能更好地捕捉长序列关联，并通过并行化实现更快训练。

典型的语言Transformer包括BERT[29]与GPT[50]，分别基于掩码语言建模与自回归学习（即下一词预测）。



### 视觉与语言模型的融合

视觉与语言领域预训练模型的进步，极大推动了VLM的发展——这类模型在统一框架中集成了视觉与语言两种模态。

早期的开创性工作（如DeViSE[15]）试图通过共享嵌入对齐视觉与文本特征，以弥合模态鸿沟，但这些模型受限于可扩展性不足，无法处理大规模、多样化的数据集。



CLIP[17]与ALIGN[51]等模型的出现彻底重塑了VLM格局，它们通过大批次对比学习训练双编码器架构，生成图像与文本的联合特征表示。

这些模型在零样本识别任务中展现出前所未有的泛化能力，解锁了图像分类、检索，甚至文本生成图像[10]等多样化下游应用的潜力。



随着Flamingo[20]与GPT-4V[52]等更强大、更通用的VLM问世，该领域持续发展。

例如，Flamingo的构建方式是：先将预训练视觉模型与预训练语言模型结合，再用图像描述数据微调组合后的模型。

最终模型在图像描述、视觉问答等感知与推理任务上表现出色，凸显了其融合视觉与语言的能力。

GPT-4V（推测以类似方式构建）则展现出更强大的感知与推理能力。

随着VLM的持续演进，它们有望创造出更直观、更通用的AI系统，使其能更像人类一样理解和与世界交互，以更贴近人类认知的方式处理和关联视觉与语言信息。



## 1.2.2 核心组件

本节讨论VLM处理、对齐与融合视觉和文本信息的主要构建模块，分为特征表示、特征对齐与融合、训练目标和数据四类。


### 特征表示

现代VLM依赖专用架构从图像与文本中提取紧凑特征。



**视觉方面**，常用的骨干网络包括擅长提取空间层次特征的CNN（如ResNet[2]），以及将图像视为块序列、通过自注意力捕捉全局依赖的ViT[32]。

这些骨干网络通常在ImageNet[31]等大规模数据集上预训练，以学习通用视觉特征。



**语言方面**，基于Transformer的架构占据主导地位，已被证明在处理序列数据时极为高效[49]。

语言模型既可与视觉模型联合训练，也可先在文本数据集上通过自监督学习预训练，再与视觉模型结合进行进一步微调。

例如，在CLIP[17]中，语言模型是与视觉模型在海量图像-文本对数据上联合训练的Transformer；

而Flamingo[20]与LLaVA[18]等模型则采用预训练语言模型，并将文本输出与视觉特征关联。



### 特征对齐与融合

弥合视觉与语言的鸿沟需要专用机制来对齐与融合不同模态的特征，主流架构有三类：

- **双编码器架构**：使用独立编码器分别处理视觉与文本信息，并通过对比学习等方式在共享嵌入空间中对齐两种模态（如CLIP[17]）。

- **交叉注意力架构**：实现视觉与文本特征的直接交互，使模型能对两种模态进行联合推理。交叉注意力通常应用于语言Transformer的中间自注意力层，代表工作包括Flamingo[20]与InstructBLIP[53]。

- **统一架构**：在同一Transformer模型中处理两种模态（如UNITER[54]）。具体做法是将图像与文本都进行标记化，再将其特征拼接成一个序列。



### 训练目标

训练目标在对齐不同模态的过程中起着关键作用，主流选择包括：

- **对比学习**：通过最大化图像-文本对在联合嵌入空间中的相似度来对齐二者（如CLIP[17]）。

- **掩码建模**：代表性方法包括掩码图像建模[30]与掩码语言建模[29]，核心思想是训练模型预测两种模态中的掩码标记（如UNITER[54]、BLIP[55]与VideoBERT[56]）。

- **自回归学习**：类似语言建模，目标是根据前文标记预测下一个标记。在VLM中，该范式常结合图像描述数据使用，即训练模型为图像生成描述（如LLaVA[18]与Flamingo[20]）。



### 数据

数据是VLM的基础，为模型提供学习世界知识的必要信息。

关键数据类型包括图像-文本对、视觉问答（VQA）数据集、指令与对话数据，以及视频-文本数据。

每种数据类型针对特定能力（从描述、推理到时序理解），共同推动多模态学习的发展。

以下是更详细的讨论：

- **图像-文本对数据**：提供对齐的视觉与文本输入，用于学习多模态表示。中等规模数据集（如COCO Captions[57]、Flickr30k[58]）为数十万张图像提供人工撰写的描述；

  大规模数据集（如LAION-5B[22]）则包含数十亿个网络收集的图像-文本对。这些数据集支撑了图像描述、检索等任务，极大助力了CLIP[17]、ALIGN[51]等VLM（它们通过对比学习将图像与文本映射到共享空间）。

- **VQA数据**：VQA数据集（如VQA v2[59]、OK-VQA[60]、GQA[61]）通过让模型用自然语言回答图像相关问题，评估其图像推理能力。这些数据集推动模型整合视觉理解、物体关系与常识推理能力。


- **指令与对话数据**：这类数据连接感知与交互，支持多模态助手、任务导向推理等应用。例如，LLaVA-Instruct[18]提供基于视觉输入的多模态指令；

  Visual Dialog[62]引入对话式推理；RefCOCO/RefCOCOg[63]聚焦通过语言识别物体。
- **视频-文本数据**：视频-文本数据集将VLM拓展到时序领域，将视频与描述、标注或字幕配对。例如，HowTo100M[64]将教学视频与字幕配对，YouCook2[65]提供分步视频标注。
# 1.3 应对复杂挑战
## 1.3.1 算法层面的挑战
构建高性能的视觉-语言模型并非易事，需要解决诸多算法层面的挑战。本节将深入探讨这些挑战，剖析算法创新核心背后的复杂性与机遇。

### 弥合模态鸿沟
视觉-语言建模的核心挑战之一，是弥合视觉与语言两种模态之间的鸿沟。具体而言，图像和视频通过像素编码空间与感知信息，而语言本质上具有符号性和抽象性。高性能的模型必须调和这些差异，构建能够捕捉两种模态核心特征的统一特征表示。现有研究中，研究者通常通过**大规模对比学习**等方法学习联合嵌入空间，以实现模态间的对齐[17,51]。但要实现鲁棒的模态对齐仍存在诸多难点，在处理模糊或稀疏数据时尤为突出。例如，英文单词“jaguar”既可以指美洲豹这一动物，也可以指代捷豹汽车，模型需要结合视觉上下文与语言线索才能准确判断其含义。

### 模型架构设计
为多模态任务量身设计模型架构是另一项重要挑战。与单模态模型不同，视觉-语言模型需要处理并融合来自完全不同数据源的信息。近年来，以CLIP为代表的模型[17]采用模态专属编码器学习共享隐空间，展现出良好的应用前景。但针对视觉-语言任务优化这类架构时，需要重点关注跨模态交互的设计。Transformer的出现彻底改变了多模态学习的格局——各类数据均可被转化为序列形式，再由自注意力机制统一处理[49]。但Transformer也存在显著缺陷，其计算和内存开销大，时间复杂度与序列长度呈二次方关系，处理超长序列时效率极低[66,67]。

### 面向特定任务的适配
视觉-语言模型参数量庞大，且存在过拟合风险，因此将其适配到下游任务面临诸多挑战。具体来说，视觉-语言模型的参数量通常达数亿级别，而下游任务的数据集规模往往适中；加之预训练与微调数据集之间存在领域鸿沟，进一步增加了适配难度。目前主流的解决方案是采用**参数高效微调方法**，如提示学习[28]、适配器调优[68]等，但这类方法常被诟病泛化能力较弱[69]。测试阶段提示调优虽能提升泛化性，却会增加计算开销[70]。除泛化性问题外，将基于图像的视觉-语言模型适配到视频、医学影像甚至三维数据等场景时，研究者还需要为其开发特定的任务模块。

### 可解释性
在视觉-语言模型的研究中，**可解释性**包含两层含义：一是设计机制阐释模型如何处理并融合多模态信息，二是为模型的预测结果生成人类可理解的推理依据。随着视觉-语言模型越来越多地应用于高敏感度、高风险场景，对其可解释性的需求也日益迫切——用户和相关利益方必须能够理解并信任模型做出的决策。但该方向的研究进展远落后于模型本身的发展，传统的显著性图等方法[71]在视觉-语言模型中并不适用：这类可视化方法仅能展示模型的关注区域，无法解释模型的决策逻辑，也回答不了“模型为何认为该图像区域至关重要”的问题。

### 持续学习能力
视觉-语言模型要适应现实场景中动态变化的需求，同时不遗忘已习得的知识，就必须具备持续学习能力。例如，部署在自动驾驶车辆中的视觉-语言模型，需要持续更新对新交通标识、道路设施变化和动态环境的理解[9]。但实现这种自适应能力绝非易事：对视觉-语言模型进行增量更新时，会面临计算和内存需求持续增加、可扩展性保障、新旧数据分布失衡等问题。推动视觉-语言模型的持续学习研究，需要创新的解决思路，包括设计内存高效的架构、制定选择性微调策略，以及研发能在“保留已有知识”和“无缝融合新信息”之间实现精细平衡的算法。

### 闭源模型的局限性
闭源模型为视觉-语言模型的定制化、透明性和研究可复现性带来了巨大挑战。目前许多性能领先的模型（如GPT-4o及其他商业系统）均不对外开放模型权重和详细架构。尽管这些模型性能优异，但其闭源属性使其难以适配特定领域的任务。这一局限性也推动了黑箱机器学习技术的研究[72,73]——用户只能通过应用程序接口与模型交互，无法查看底层参数和中间计算过程。由于无法获取模型权重，开发者往往只能通过耗时费力、成本高昂的试错法进行提示工程，且最终效果往往不尽如人意。此外，模型的黑箱属性还加剧了调试、结果解读和偏置修正的难度，严重阻碍了其更广泛的应用和落地可信度。

## 1.3.2 计算层面的挑战
构建视觉-语言模型的计算开销极大，模型的训练、推理和部署均需要大量计算资源。解决这些计算挑战，是视觉-语言模型实现规模化发展，同时保证效率、可及性和可持续性的关键。本节将深入分析核心的计算层面挑战。

### 训练与推理的可扩展性
现代视觉-语言模型的参数量达数十亿级别，这一前所未有的规模使其在训练和推理阶段面临严峻的可扩展性问题。训练这类模型需要海量计算资源，通常依赖高性能图形处理器、张量处理器和大容量内存。例如，训练类GPT-4的模型系统，需要数千个加速器持续运算数周，成本高达数百万美元[52]。推理阶段的计算需求同样居高不下，尤其是实时应用场景：每一次查询都需要运行大模型，带来巨大的内存和计算开销。研究者通常会采用各类内存优化方法（如零阶优化器[74]）降低训练阶段的内存占用，并通过量化技术[75,76]压缩模型权重，以方便模型部署。

### 长序列与高分辨率输入的处理
视觉-语言模型常需处理高维输入，如长视频、高分辨率图像等，这会显著增加计算成本。例如，分析一部完整的影片、处理十亿像素级的医学影像，均需要大量的内存和计算资源。而视觉-语言模型最主流的骨干架构Transformer，其复杂度与序列长度呈二次方关系，处理长序列时效率极低。

### 延迟与实时性约束
视觉-语言模型的诸多应用场景（如自动驾驶、交互式助手、视频分析）均要求推理过程满足实时性或低延迟要求。但大模型的高计算需求往往会导致推理延迟，使其无法适用于对延迟敏感的任务。模型缓存、自适应推理、边缘计算等优化手段虽能缩短响应时间，却会为模型部署增加复杂度。

### 分布式与联邦学习
在由多个计算节点组成的分布式环境中训练视觉-语言模型，会面临节点协调、同步和通信开销等挑战。梯度更新需要在各节点间高效传输，在大规模分布式训练中，这一过程极易成为性能瓶颈。联邦学习则进一步增加了训练的复杂性：该方法采用去中心化的训练模式，数据保留在本地设备中，以保障数据隐私，非常适合处理医疗影像、个人照片等敏感多模态数据，但对训练框架的设计提出了更高要求。

## 1.3.3 数据层面的挑战
高质量、多样化的数据集是视觉-语言模型研发成功的关键，但这类数据集的收集、构建和使用面临诸多挑战，直接影响模型的性能和泛化能力。本节将探讨构建视觉-语言模型时面临的核心数据挑战。

### 数据集的多样性与代表性
视觉-语言模型的性能高度依赖训练数据的多样性和代表性。多模态数据集必须覆盖广泛的视觉和语言上下文，才能让模型在不同领域和任务中具备良好的泛化性。但目前大多数公开数据集均存在偏置，往往倾向于特定地理区域、文化或领域（如以西方为中心的图像数据[77]），这种偏置会导致模型在数据代表性不足的场景中失效。要解决这一问题，需要构建更具包容性、能代表全球多元特征的数据集。

### 多模态数据的规模与质量
多模态数据集的规模扩张往往以牺牲质量为代价。例如，LAION-5B数据集[22]从多源收集了海量图像-文本对，但其中的文本描述存在噪声、不完整或与图像无关等问题。基于这类数据训练模型，易导致错误和偏置在模型中传递。对大规模多模态数据集进行清洗和过滤并非易事，需要构建鲁棒的自动化处理流程，或投入大量人力进行审核，二者均需耗费巨大资源。

### 长尾与稀有概念的捕捉
视觉-语言模型的一大常见挑战，是捕捉数据集中出现频率极低的**长尾概念和稀有概念**。例如，在野生动物图像中识别稀有物种、理解科技文本中的小众专业术语，都需要模型在训练阶段接触到相关样本。但数据分布的不均衡性，往往导致模型对常见概念过拟合，而在稀有概念上表现不佳[78]。目前潜在的解决方案包括数据增强、均衡采样和合成数据生成等。

### 伦理与隐私问题
多模态数据的收集往往需要从互联网爬取内容，这引发了严重的伦理和隐私问题。数据集中可能会无意间包含敏感或隐私信息，如个人照片、位置元数据、受版权保护的内容，且这些内容的使用往往未获得相关主体的明确授权。此外，缺乏规范的数据集易导致偏置或不良内容的传递，使模型在医疗、执法等敏感应用场景中产生有害输出。为解决这些问题，开发者应制定清晰的数据收集和标注准则，并通过详细的数据集文档提升透明度。遵循伦理的数据处理规范，是构建具有社会责任感、可被信任的人工智能系统的核心要求。

